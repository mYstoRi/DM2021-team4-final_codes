{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwrgVt461noN",
        "outputId": "534c3bd3-ca79-4ff5-dca1-af8df40c3c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (0.42.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install jieba\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ya7PYumW1noY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "import numpy as np\n",
        "import math\n",
        "from csv import reader\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import os\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YoJbeAC61noa"
      },
      "outputs": [],
      "source": [
        "\n",
        "with zipfile.ZipFile('dataTrainComplete.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/')\n",
        "\n",
        "with zipfile.ZipFile('dataPrivateComplete.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxABBH6c1nob",
        "outputId": "b9c951be-5ec2-44fa-ee5f-4001491178b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['733', '776', '1190', '654', '916', '1366', '276', '219', '273', '1357']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "filenames = [\".\".join(f.split(\".\")[:-1]) for f in os.listdir('data/dataTrainComplete') if os.path.isfile(os.path.join('data/dataTrainComplete', f))]\n",
        "filenames[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggonMLa41noc",
        "outputId": "e24912c4-1b70-4dbb-b2e5-6ad654615440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('3', '415'), ('3', '649'), ('9', '5'), ('25', '32'), ('25', '41'), ('26', '37'), ('27', '46'), ('29', '72'), ('32', '25'), ('32', '41')]\n"
          ]
        }
      ],
      "source": [
        "with open('data/TrainLabel.csv', 'r') as f:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(f)\n",
        "    # Get all rows of csv from csv_reader object as list of tuples\n",
        "    doc_ref_tuples = list(map(tuple, csv_reader))[1:]\n",
        "    # display all rows of csv\n",
        "    print(doc_ref_tuples[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2et_MWvq1noe"
      },
      "outputs": [],
      "source": [
        "doc_pairs = list(itertools.permutations(filenames,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0PZXJ9h1noe",
        "outputId": "79a7d671-9064-421a-9ca2-fbcc6b307e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of files:  560\n",
            "number of similar files:  1383\n",
            "number of all possible pairs:  313040\n",
            "ratio similar/dissimilar pairs:  0.004417965755175057\n"
          ]
        }
      ],
      "source": [
        "# data is highly imbalanced\n",
        "print(\"number of files: \", len(filenames))\n",
        "print(\"number of similar files: \", len(doc_ref_tuples))\n",
        "print(\"number of all possible pairs: \", len(doc_pairs))\n",
        "print(\"ratio similar/dissimilar pairs: \", len(doc_ref_tuples)/len(doc_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_ZPFeFDg1noh"
      },
      "outputs": [],
      "source": [
        "texts = []\n",
        "for i, file in enumerate(filenames):\n",
        "        with open('data/dataTrainComplete/' + file + '.txt', 'r', encoding='UTF-8') as f:\n",
        "            f = f.read()\n",
        "            texts.append(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hBjZMG3v1noi"
      },
      "outputs": [],
      "source": [
        "# prepare training labels: 1 -> if docs are similar, 0 -> not similar\n",
        "y_train = []\n",
        "for pair in doc_pairs:\n",
        "    if pair in doc_ref_tuples:\n",
        "        y_train.append(1)\n",
        "    else:\n",
        "        y_train.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_file = open(\"data/features.txt\", \"r\")\n",
        "features = features_file.read().splitlines()\n",
        "features[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8gKbvCTl-O0",
        "outputId": "dfaf7f18-be77-43a7-db32-ebe82ff361da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['甜菜夜蛾', '黑點病', '軟腐病']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "BCOGehNxl_LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jieba.load_userdict('data/features.txt') "
      ],
      "metadata": {
        "id": "3sTylsNPl-SP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for t in texts:\n",
        "  tokens.append(jieba.lcut(t))"
      ],
      "metadata": {
        "id": "lMH0_s66l-X4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens[0]"
      ],
      "metadata": {
        "id": "_aP_8No6m4P8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use Python NLP toolkit to analyze Traditional Chinese text, CKIP is your first choice. CKIP is developed by Taiwan Institute of Information Science, Academia Sinica, And won rankings in many competitions."
      ],
      "metadata": {
        "id": "_mAO0xnynHZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ckiptagger\n",
        "!pip3 install tensorflow\n",
        "!pip3 install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn6FZU9-nIzq",
        "outputId": "a50247bc-c6ae-4461-8b9a-c17a37b9a600"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ckiptagger\n",
            "  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from ckiptagger import data_utils\n",
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlmdUJu-ncrT",
        "outputId": "1fefdce9-e085-49a1-9e8d-ce40459ba7ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "100%|██████████| 1.88G/1.88G [00:18<00:00, 102MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
        "import os"
      ],
      "metadata": {
        "id": "vGCHXfHhnd6h"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ws = WS(\"./data\")\n",
        "#pos = POS(\"./data\")\n",
        "#ner = NER(\"./data\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "ws = WS(\"./data\", disable_cuda=False)\n",
        "pos = POS(\"./data\", disable_cuda=False)\n",
        "ner = NER(\"./data\", disable_cuda=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJAcLJP_n6Cj",
        "outputId": "9abc4d03-d027-4ed5-db1f-cda61c176ccd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dct = {features[i]: 1 for i in range(len(features))}\n",
        "feature_dict = construct_dictionary(feature_dct)\n",
        "feature_dict[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heeSSepGrzu9",
        "outputId": "a73d5bf8-19d3-411c-f511-73b60f1274f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " {' ': 1.0,\n",
              "  '桃': 1.0,\n",
              "  '桑': 1.0,\n",
              "  '梅': 1.0,\n",
              "  '梨': 1.0,\n",
              "  '棗': 1.0,\n",
              "  '茶': 1.0,\n",
              "  '蔥': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list = ws(\n",
        "    texts,\n",
        "    # sentence_segmentation = True, # To consider delimiters\n",
        "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
        "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
        "    coerce_dictionary = feature_dict # words in this dictionary are forced\n",
        ")"
      ],
      "metadata": {
        "id": "iwX7tJ72p-V8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_chars = ['，', '\\n', '(', ')','。','、','：','\\n\\n','~','.','；','\\u3000', '～', '℃', '（','）']"
      ],
      "metadata": {
        "id": "DrPxfxPbub-M"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_word_sentence_list = [[] for i in range(len(word_sentence_list))]\n",
        "\n",
        "for i, tokens in enumerate(word_sentence_list):\n",
        "  for t in tokens:\n",
        "    if t not in special_chars and not t.isdigit() and \"％\" not in t and \"%\" not in t and \".\" not in t and \",\" not in t:\n",
        "      clean_word_sentence_list[i].append(t)"
      ],
      "metadata": {
        "id": "6gb5huKHqyWn"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chem = pd.read_excel('data/Keywords/02chem.list.xlsx', engine='openpyxl', header=None)\n",
        "crop = pd.read_excel('data/Keywords/02crop.list.xlsx', engine='openpyxl', header=None)\n",
        "pest = pd.read_excel('data/Keywords/02pest.list.xlsx', engine='openpyxl', header=None)\n",
        "\n",
        "rows = len(chem)\n",
        "columns = len(chem.columns)\n",
        "chems_syn = [[] for i in range(rows)]\n",
        "for r in range(rows):\n",
        "    for c in range(columns):\n",
        "        if type(chem.iloc[r,c]) == str:\n",
        "            chems_syn[r].append(chem.iloc[r,c])\n",
        "\n",
        "\n",
        "rows = len(crop)\n",
        "columns = len(crop.columns)\n",
        "crops_syn = [[] for i in range(rows)]\n",
        "for r in range(rows):\n",
        "    for c in range(columns):\n",
        "        if type(crop.iloc[r,c]) == str:\n",
        "            crops_syn[r].append(crop.iloc[r,c])\n",
        "\n",
        "\n",
        "rows = len(pest)\n",
        "columns = len(pest.columns)\n",
        "pests_syn = [[] for i in range(rows)]\n",
        "for r in range(rows):\n",
        "    for c in range(columns):\n",
        "        if type(pest.iloc[r,c]) == str:\n",
        "            pests_syn[r].append(pest.iloc[r,c])"
      ],
      "metadata": {
        "id": "SjZdYSqWzToA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_keywords = chems_syn+crops_syn+pests_syn"
      ],
      "metadata": {
        "id": "47Yag54U1vFG"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_clean_tokens = clean_word_sentence_list.copy()\n",
        "# replace by synonym\n",
        "for i, tokens in enumerate(clean_word_sentence_list):\n",
        "  for j, t in enumerate(tokens):\n",
        "    for kw in all_keywords:\n",
        "      if t in kw:\n",
        "        syn_clean_tokens[i][j] = kw[0] #always choose first synonym"
      ],
      "metadata": {
        "id": "XbFLA1QQuAY1"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "Mjob_WZkS6Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "Q2kueOncS7eH"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(testy, yhat_classes):\n",
        "    # accuracy: (tp + tn) / (p + n)\n",
        "    accuracy = accuracy_score(testy, yhat_classes)\n",
        "    print('Accuracy: %f' % accuracy)\n",
        "    # precision tp / (tp + fp)\n",
        "    precision = precision_score(testy, yhat_classes)\n",
        "    print('Precision: %f' % precision)\n",
        "    # recall: tp / (tp + fn)\n",
        "    recall = recall_score(testy, yhat_classes)\n",
        "    print('Recall: %f' % recall)\n",
        "    # f1: 2 tp / (2 tp + fp + fn)\n",
        "    f1 = f1_score(testy, yhat_classes)\n",
        "    print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "id": "4UkAb_SdS9a4"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# keyword set"
      ],
      "metadata": {
        "id": "DuIKKzkduAoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kw_sets = [set() for i in range(len(syn_clean_tokens))]\n",
        "for i, tokens in enumerate(syn_clean_tokens):\n",
        "  for t in tokens:\n",
        "    if t in features:\n",
        "      kw_sets[i].add(t)"
      ],
      "metadata": {
        "id": "SFvcv-AGuC3r"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_sets[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zKqVA6T4dYQ",
        "outputId": "6a1bfc59-a670-4b67-bb0a-e6bf790fbbba"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'柑桔', '白柚', '賽洛寧', '陶斯松'}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1hR7eVo55izm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(s1, s2):\n",
        "    if len(s1.union(s2)) == 0: return 0\n",
        "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))"
      ],
      "metadata": {
        "id": "RZTc8J0H4dc4"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jaccard_scores = []\n",
        "for (d1,d2) in doc_pairs:\n",
        "  i1 = filenames.index(d1)\n",
        "  i2 = filenames.index(d2)\n",
        "\n",
        "  jaccard_scores.append(jaccard_similarity(kw_sets[i1], kw_sets[i2]))"
      ],
      "metadata": {
        "id": "_HkyRq455ESG"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(x == 0 for x in jaccard_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V0u3Wup5eSg",
        "outputId": "b937c038-35f0-49a9-980b-439fb0f21f13"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "211778"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred10 = [0]*len(doc_pairs)\n",
        "#if pair has common keywords in all categories -> classify as similar\n",
        "for i, pair in enumerate(doc_pairs):\n",
        "  if jaccard_scores[i] >= 0.7:\n",
        "    y_pred10[i] = 1\n",
        "        \n",
        "get_metrics(y_train, y_pred10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3cpfV7s6H-W",
        "outputId": "41719045-2388-4f8e-edee-e0482d29ab0e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.995895\n",
            "Precision: 0.545037\n",
            "Recall: 0.428778\n",
            "F1 score: 0.479968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sorensin_similarity(s1, s2):\n",
        "    if (len(s1)+len(s1))==0 : return 0\n",
        "    return float(2*len(s1.intersection(s2)) / (len(s1)+len(s1)))"
      ],
      "metadata": {
        "id": "bPGdSY_gArbR"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorensin_scores = []\n",
        "for (d1,d2) in doc_pairs:\n",
        "  i1 = filenames.index(d1)\n",
        "  i2 = filenames.index(d2)\n",
        "\n",
        "  sorensin_scores.append(sorensin_similarity(kw_sets[i1], kw_sets[i2]))"
      ],
      "metadata": {
        "id": "rQ0DswYNAijq"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred12 = [0]*len(doc_pairs)\n",
        "#if pair has common keywords in all categories -> classify as similar\n",
        "for i, pair in enumerate(doc_pairs):\n",
        "  if sorensin_scores[i] >= 0.97: #0.609772\n",
        "    y_pred12[i] = 1\n",
        "        \n",
        "get_metrics(y_train, y_pred12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FJRGsO-BB4j",
        "outputId": "444d04ca-85e6-4b10-c426-443c77936056"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.996173\n",
            "Precision: 0.554831\n",
            "Recall: 0.676790\n",
            "F1 score: 0.609772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overlap_similarity(s1, s2):\n",
        "    if len(s1) == 0 or len(s2)==0 : return 0\n",
        "    return float(len(s1.intersection(s2)) / min(len(s1),len(s1)))"
      ],
      "metadata": {
        "id": "I6pkFmSABw5n"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overlap_scores = []\n",
        "for (d1,d2) in doc_pairs:\n",
        "  i1 = filenames.index(d1)\n",
        "  i2 = filenames.index(d2)\n",
        "\n",
        "  overlap_scores.append(overlap_similarity(kw_sets[i1], kw_sets[i2]))"
      ],
      "metadata": {
        "id": "Aicb6bHqCnJQ"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred13 = [0]*len(doc_pairs)\n",
        "#if pair has common keywords in all categories -> classify as similar\n",
        "for i, pair in enumerate(doc_pairs):\n",
        "  if overlap_scores[i] >= 0.97: #0.609772\n",
        "    y_pred13[i] = 1\n",
        "        \n",
        "get_metrics(y_train, y_pred13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrg963cnCtHy",
        "outputId": "5b23a574-c235-4ff7-a1eb-2aa35183e131"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.996173\n",
            "Precision: 0.554831\n",
            "Recall: 0.676790\n",
            "F1 score: 0.609772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def twersky_similarity(s1, s2):\n",
        "  a = 0.97\n",
        "  b= 0.03\n",
        "  if len(s1) == 0 or len(s2)==0 : return 0\n",
        "  return float(len(s1.intersection(s2)) / (len(s1.intersection(s2))+a*len(s1.difference(s2))+b*len(s2.difference(s1))))"
      ],
      "metadata": {
        "id": "w17YYe7TGZIf"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twersky_scores = []\n",
        "for (d1,d2) in doc_pairs:\n",
        "  i1 = filenames.index(d1)\n",
        "  i2 = filenames.index(d2)\n",
        "\n",
        "  twersky_scores.append(twersky_similarity(kw_sets[i1], kw_sets[i2]))"
      ],
      "metadata": {
        "id": "RLYJemluH6oO"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred15 = [0]*len(doc_pairs)\n",
        "#if pair has common keywords in all categories -> classify as similar\n",
        "for i, pair in enumerate(doc_pairs):\n",
        "  if twersky_scores[i] >= 0.92: #0.665923 0.92 0.97 for a\n",
        "    y_pred15[i] = 1\n",
        "        \n",
        "get_metrics(y_train, y_pred15)  # Good score here but when uploading, we get 0.0037950"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHWNus-nIKC-",
        "outputId": "1ed324cf-deb7-483a-a68d-ac6f1c09dc36"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997131\n",
            "Precision: 0.685824\n",
            "Recall: 0.647144\n",
            "F1 score: 0.665923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf idf"
      ],
      "metadata": {
        "id": "DpnWt-Wr8Ae-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "CQ1sX52oTOKS"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doctoanalyze = []\n",
        "for tokens in syn_clean_tokens:\n",
        "  doc_with_space=''\n",
        "  for token in tokens:\n",
        "      doc_with_space += token\n",
        "      doc_with_space += ' '\n",
        "  doctoanalyze.append(doc_with_space)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(doctoanalyze)\n",
        "\n",
        "cosinescore = cosine_similarity(tfidf_matrix,tfidf_matrix)"
      ],
      "metadata": {
        "id": "3ErVZCBL6ICF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred11 = [0]*len(doc_pairs)\n",
        "#if pair has common keywords in all categories -> classify as similar\n",
        "for i, (d1,d2) in enumerate(doc_pairs):\n",
        "  idx1 = filenames.index(d1)\n",
        "  idx2 = filenames.index(d2)\n",
        "  if cosinescore[idx1][idx2] >= 0.2 and jaccard_scores[i] >= 0.7:\n",
        "    y_pred11[i] = 1\n",
        "        \n",
        "get_metrics(y_train, y_pred11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEIzMzRh57hZ",
        "outputId": "9cf0dbba-d2e3-4835-f760-6b9c3209c895"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.995914\n",
            "Precision: 0.548327\n",
            "Recall: 0.426609\n",
            "F1 score: 0.479870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VjzBC-U2DDFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rnSRdFEIDDKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hVMyBYH0DDPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test on private data"
      ],
      "metadata": {
        "id": "JgiClp5WDD9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('dataPrivateComplete.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/')"
      ],
      "metadata": {
        "id": "PpJGauMgDg9w"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames_test = [\".\".join(f.split(\".\")[:-1]) for f in os.listdir('data/dataPrivateComplete') if os.path.isfile(os.path.join('data/dataPrivateComplete', f))]\n",
        "filenames_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icyIsFT8DhBz",
        "outputId": "74b1e964-53d4-4562-9ed2-4e29f64ed8b7"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['768', '1171', '166', '322', '1224', '708', '674', '976', '1211', '1003']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_pairs_test = list(itertools.permutations(filenames_test,2))\n"
      ],
      "metadata": {
        "id": "rBleweEiDhG_"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_test = []\n",
        "for i, file in enumerate(filenames_test):\n",
        "        with open('data/dataPrivateComplete/' + file + '.txt', 'r', encoding='UTF-8') as f:\n",
        "            f = f.read()\n",
        "            texts_test.append(f)"
      ],
      "metadata": {
        "id": "kf25GdUMDhL3"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list_test = ws(\n",
        "    texts_test,\n",
        "    # sentence_segmentation = True, # To consider delimiters\n",
        "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
        "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
        "    coerce_dictionary = feature_dict # words in this dictionary are forced\n",
        ")"
      ],
      "metadata": {
        "id": "UoVJw2PgEOI-"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_word_sentence_list_test = [[] for i in range(len(word_sentence_list_test))]\n",
        "\n",
        "for i, tokens in enumerate(word_sentence_list_test):\n",
        "  for t in tokens:\n",
        "    if t not in special_chars and not t.isdigit() and \"％\" not in t and \"%\" not in t and \".\" not in t and \",\" not in t:\n",
        "      clean_word_sentence_list_test[i].append(t)"
      ],
      "metadata": {
        "id": "yOoaPFd8EOTO"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_clean_tokens_test = clean_word_sentence_list_test.copy()\n",
        "# replace by synonym\n",
        "for i, tokens in enumerate(clean_word_sentence_list_test):\n",
        "  for j, t in enumerate(tokens):\n",
        "    for kw in all_keywords:\n",
        "      if t in kw:\n",
        "        syn_clean_tokens_test[i][j] = kw[0] #always choose first synonym"
      ],
      "metadata": {
        "id": "GPYnuc3sDo3-"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_sets_test = [set() for i in range(len(syn_clean_tokens_test))]\n",
        "for i, tokens in enumerate(syn_clean_tokens_test):\n",
        "  for t in tokens:\n",
        "    if t in features:\n",
        "      kw_sets_test[i].add(t)"
      ],
      "metadata": {
        "id": "JKH_NRQKDo9A"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorensin_scores_test = []\n",
        "#twersky_scores_test = []\n",
        "for (d1,d2) in doc_pairs_test:\n",
        "  i1 = filenames_test.index(d1)\n",
        "  i2 = filenames_test.index(d2)\n",
        "\n",
        "  sorensin_scores_test.append(sorensin_similarity(kw_sets_test[i1], kw_sets_test[i2]))"
      ],
      "metadata": {
        "id": "_HRIKKi0E0SC"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred14 = [0]*len(doc_pairs_test)\n",
        "#if pair has common keywords in all categories -> classify as similar\n",
        "for i, pair in enumerate(doc_pairs_test):\n",
        "  if sorensin_scores_test[i] >= 0.97: #0.609772 #twersky_scores[i] >= 0.92:\n",
        "    y_pred14[i] = 1"
      ],
      "metadata": {
        "id": "xbVvo29DEx2m"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_test = []\n",
        "\n",
        "#y_pred_nb_test = nb.predict(common_kw_list_test)\n",
        "\n",
        "for i, pair in enumerate(doc_pairs_test):\n",
        "  if y_pred14[i] == 1: #clf_under.predict([X_test[i]]) == 1:\n",
        "      res_test.append(pair)"
      ],
      "metadata": {
        "id": "QomNtaYcDGN7"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = [['Test','Reference']] + res_test\n",
        "            \n",
        "import csv\n",
        "\n",
        "with open('data/test_result.csv', 'w', encoding='UTF-8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for pair in test_gen:\n",
        "        writer.writerow(pair)"
      ],
      "metadata": {
        "id": "YmWdPnTEFJ7o"
      },
      "execution_count": 277,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "similarity metrics on keyword sets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R_SAGmGm1npI"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}